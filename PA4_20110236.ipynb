{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Spring 2020\n",
    "### Naive Bayes Classifier\n",
    "### Ahmed Anwar 20110236"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import re\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(review, stops = stops):\n",
    "    review = review.lower().replace(',','').replace('.','').replace(':','').replace('/><br','').replace('/>','').replace(')','').replace('(','')\n",
    "    review = review.replace((\"\\'\"),(\"\")).replace(\";\",\"\").replace('  ',' ').replace('?',' ').replace('\"', '').replace('!','').replace('  ', ' ').replace('+','')\n",
    "    review = \" \".join(filter(lambda x:x[0]!='@', review.split())) #removes @\n",
    "    review = re.sub(r'[0-9]+', '', review) #removes numbers\n",
    "    review = review.encode('ascii', 'ignore').decode('ascii') #removes emojis\n",
    "    review = ' '.join([word for word in re.split(\"\\W+\", review) if word not in stops])\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@USAirways Is there a phone line to call into ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@united Bag was finally delivered and intact. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>@usairways Thanks to Kevin and team at F38ish ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir Yes, talked to them. FLL says is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral  @USAirways Is there a phone line to call into ...\n",
       "1          positive  @united Bag was finally delivered and intact. ...\n",
       "2          positive  @usairways Thanks to Kevin and team at F38ish ...\n",
       "3          negative  @AmericanAir Yes, talked to them. FLL says is ...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('tweets.csv')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d5cdf3aaa44ca5bab9997ca47a6418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14640.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(tweets))):\n",
    "    tweets.text[i] = clean_tweet(tweets.text[i])\n",
    "    time.sleep(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = tweets.groupby('airline_sentiment').apply(lambda x: x.sample(int(0.80*len(x))))\n",
    "grp = grp.rename(columns = {'airline_sentiment' : 'sentiment'}).reset_index()\n",
    "training_set = grp.rename(columns = {'level_1':'Document'}).drop(columns=['airline_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_indices = list(range(0,len(tweets)))\n",
    "train_indices = training_set.Document.tolist()\n",
    "test_indices = [x for x in total_indices if x not in train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = training_set.drop(columns=['Document'])\n",
    "test_set = tweets.iloc[test_indices,].reset_index(drop=True).rename(columns={'airline_sentiment':'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    1836\n",
       "neutral      620\n",
       "positive     473\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    7342\n",
       "neutral     2479\n",
       "positive    1890\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus of words for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(df, Class):\n",
    "    corpus = []\n",
    "    sentences = df[df.sentiment==Class].text\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            corpus.append(word)\n",
    "    return (corpus, len(sentences)) #returns a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each contains corpus of words and frequency of each class\n",
    "neg_class = create_corpus(train_set,'negative')\n",
    "pos_class = create_corpus(train_set,'positive')\n",
    "neu_class = create_corpus(train_set,'neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabubulary of unique words in whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab_unique = []\n",
    "for word in neg_class[0]:\n",
    "    if word not in Vocab_unique:\n",
    "        Vocab_unique.append(word)\n",
    "        \n",
    "for word in pos_class[0]:\n",
    "    if word not in Vocab_unique:\n",
    "        Vocab_unique.append(word)\n",
    "        \n",
    "for word in neu_class[0]:\n",
    "    if word not in Vocab_unique:\n",
    "        Vocab_unique.append(word)\n",
    "\n",
    "N = len(Vocab_unique) #Number of unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Prior Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_neg = neg_class[1] / len(train_set)\n",
    "p_pos = pos_class[1] / len(train_set)\n",
    "p_neu = neu_class[1] / len(train_set)\n",
    "\n",
    "priors = (p_neg, p_pos, p_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unseen(example):\n",
    "    test_corpus = clean_tweet(example).split()\n",
    "    for word in test_corpus:\n",
    "        if word not in Vocab_unique:\n",
    "            test_corpus.remove(word)\n",
    "    return test_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Posterior for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(example,Class):\n",
    "    \n",
    "    class_corpus = Class[0]\n",
    "    n = len(class_corpus)\n",
    "    \n",
    "    test_corpus = remove_unseen(example) # clean and remove unseen\n",
    "    posterior = 1\n",
    "\n",
    "    for word in test_corpus:\n",
    "        posterior = posterior * (class_corpus.count(word) +1 ) / (n+N)\n",
    "    \n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(example, priors=priors):\n",
    "    \n",
    "    neg = np.log(posterior(example, neg_class) * priors[0])\n",
    "    pos = np.log(posterior(example, pos_class) * priors[1])\n",
    "    neu = np.log(posterior(example, neu_class) * priors[2])\n",
    "    \n",
    "    Argmax = np.array([neg,pos,neu]).argmax()\n",
    "    \n",
    "    if Argmax == 0:\n",
    "        return 'negative'\n",
    "    elif Argmax == 1:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_batch(df):\n",
    "    predictions = []\n",
    "    print('Predicting...')\n",
    "    for i in tqdm(range(len(df))):\n",
    "        temp = predict(df.text[i])\n",
    "        predictions.append(temp)\n",
    "        time.sleep(0.001)\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_pred,y_true):\n",
    "    cmat = np.zeros((3,3), dtype = int)\n",
    "    cmat[0][0] = ((y_pred == 'negative') & (y_true == 'negative')).sum()\n",
    "    cmat[0][1] = ((y_pred == 'negative') & (y_true == 'neutral')).sum()\n",
    "    cmat[0][2] = ((y_pred == 'negative') & (y_true == 'positive')).sum()\n",
    "\n",
    "    cmat[1][0] = ((y_pred == 'neutral') & (y_true == 'negative')).sum()\n",
    "    cmat[1][1] = ((y_pred == 'neutral') & (y_true == 'neutral')).sum()\n",
    "    cmat[1][2] = ((y_pred == 'neutral') & (y_true == 'positive')).sum()\n",
    "\n",
    "    cmat[2][0] = ((y_pred == 'positive') & (y_true == 'negative')).sum()\n",
    "    cmat[2][1] = ((y_pred == 'positive') & (y_true == 'neutral')).sum()\n",
    "    cmat[2][2] = ((y_pred == 'positive') & (y_true == 'positive')).sum()\n",
    "    \n",
    "    print('------True Labels----')\n",
    "    print('     neg', ' neu', ' pos')\n",
    "    print('neg', cmat[0])\n",
    "    print('neu', cmat[1])\n",
    "    print('pos', cmat[2])\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(y_pred,y_true):\n",
    "\n",
    "    labels = ['negative','neutral','positive']\n",
    "    Eval = []\n",
    "    for label in labels:\n",
    "        temp = []\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == label:\n",
    "                  output = 'P'\n",
    "            else:\n",
    "                  output = 'N'\n",
    "        \n",
    "            #classifier predicted positive\n",
    "            if output == 'P':\n",
    "                if y_true[i] == label:\n",
    "                    output = 'T'+ output\n",
    "                else:\n",
    "                    output = 'F' + output\n",
    "        \n",
    "            #classifier prediced negative\n",
    "            if output == 'N':\n",
    "                if y_true[i] == label:\n",
    "                    output = 'F'+ output\n",
    "                else:\n",
    "                    output = 'T' + output\n",
    "        \n",
    "            temp.append(output)\n",
    "        Eval.append(np.unique(np.array(temp), return_counts = True))   \n",
    "        \n",
    "    precision, recall, F1 = macro_avg(Eval)\n",
    "        \n",
    "    print('\\n')\n",
    "    print('Model Accuracy : ', round(100*np.sum(y_pred==y_true) / len(y_pred),2), '%')   \n",
    "    print('Precision : ', round(precision,3))\n",
    "    print('Recall    : ', round(recall,3))\n",
    "    print('F1 score  : ', round(F1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_avg(Eval):\n",
    "    Precisions = []\n",
    "    Recalls = []\n",
    "    F1s = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        FN = Eval[i][1][0]\n",
    "        FP = Eval[i][1][1]\n",
    "        TN = Eval[i][1][2]\n",
    "        TP = Eval[i][1][3]\n",
    "\n",
    "        precision = TP/(TP+FP)\n",
    "        recall =    TP/(TP+FN)\n",
    "        F1 = (2*precision*recall)/(precision+recall)\n",
    "\n",
    "        Precisions.append(precision)\n",
    "        Recalls.append(recall)\n",
    "        F1s.append(F1)\n",
    "\n",
    "    macro_precision = np.mean(Precisions)\n",
    "    macro_recall = np.mean(Recalls)\n",
    "    macro_F1 = np.mean(F1s)\n",
    "\n",
    "    return macro_precision, macro_recall, macro_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb5ebca0c5a44ef9ec1149cdf9df570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2929.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_on_batch(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------True Labels----\n",
      "     neg  neu  pos\n",
      "neg [1740  325  171]\n",
      "neu [ 74 245  52]\n",
      "pos [ 22  50 250]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix(predictions, test_set.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model Accuracy :  76.31 %\n",
      "Precision :  0.738\n",
      "Recall    :  0.624\n",
      "F1 score  :  0.659\n"
     ]
    }
   ],
   "source": [
    "model_evaluation(predictions,test_set.sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
